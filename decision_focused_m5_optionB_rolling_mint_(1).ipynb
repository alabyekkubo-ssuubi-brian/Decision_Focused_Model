{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alabyekkubo-ssuubi-brian/Decision_Focused_Model/blob/main/decision_focused_m5_optionB_rolling_mint_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WwNUuJvbF-i",
        "outputId": "f3b9f8ef-c24c-4bb9-c2b3-bce206141964"
      },
      "id": "3WwNUuJvbF-i",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# print(os.listdir('/content/drive/MyDrive'))\n",
        "print(os.listdir('/content/drive/MyDrive/m5_forecast'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf-gqs8MdYqP",
        "outputId": "4858365b-2dad-4c39-ca6b-102bcd3e3771"
      },
      "id": "yf-gqs8MdYqP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['calendar.csv', 'sales_train_evaluation.csv', 'sales_train_validation.csv', 'sell_prices.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Paths\n",
        "    data_dir: str = \"/content/drive/MyDrive/m5_forecast\"\n",
        "    output_dir: str = \"/content/drive/MyDrive/m5_forecast/results\"\n",
        "\n",
        "    # Dataset / filtering\n",
        "    state_filter: str = \"CA\"     # or \"TX\", \"WI\", or \"\" for all states\n",
        "\n",
        "    # Feature engineering\n",
        "    lags: List[int] = field(default_factory=lambda: [1, 7, 28])\n",
        "    rolling_windows: List[int] = field(default_factory=lambda: [7, 28])\n",
        "\n",
        "    # Quantiles\n",
        "    quantiles: Tuple[float, ...] = (0.1, 0.5, 0.9)\n",
        "\n",
        "    # Model hyperparameters\n",
        "    hidden_dim: int = 128\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 1024\n",
        "    learning_rate: float = 1e-3\n",
        "    epochs_baseline: int = 10\n",
        "    epochs_decision: int = 10\n",
        "    epochs_tft: int = 3\n",
        "\n",
        "    # Decision-focused\n",
        "    cost_holding: float = 1.0\n",
        "    cost_shortage: float = 4.0\n",
        "    lambda_smooth: float = 10.0  # smoothness for softplus\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JXZWXDSScg_O"
      },
      "id": "JXZWXDSScg_O",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision-Focused Probabilistic Forecasting on M5\n",
        "# Aligning Retail Demand Predictions with Inventory Cost Objectives\n",
        "#\n",
        "# Designed for Google Colab:\n",
        "# - Put the M5 files in config.data_dir:\n",
        "#       sales_train_validation.csv\n",
        "#       calendar.csv\n",
        "#       sell_prices.csv\n",
        "#\n",
        "# Sections:\n",
        "#  1. Setup & Imports + Config\n",
        "#  2. Load M5 Dataset\n",
        "#  3. Feature Engineering (+ log_demand)\n",
        "#  4. Rolling-Origin CV Definition (3 × 28-day folds)\n",
        "#  5. PyTorch Dataset + Training Utilities\n",
        "#  6. Baseline Quantile Model (Pinball Loss, on log_demand)\n",
        "#  7. Decision-Focused Model (Newsvendor Loss + Contextual τ(x), log-space, warm-start + multi-task)\n",
        "#  8. Rolling Evaluation Loop (MASE, RMSSE, Cost, Fill Rate, Stockouts, CRPS)\n",
        "#  9. Simple Hierarchical Aggregation (Bottom-Up)\n",
        "# 10. Cost Sensitivity Analysis (cs:ch)\n",
        "# 11. Temporal Fusion Transformer (TFT) Section (optional)\n",
        "# 12. Conclusion\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1. Setup & Imports + Config\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Paths\n",
        "    data_dir: str = \"/content/drive/MyDrive/m5_forecast\"  # change to your Google Drive path if needed\n",
        "    state_filter: str | None = \"CA\"  # set to None to use all states\n",
        "\n",
        "    # Training / model\n",
        "    batch_size: int = 1024\n",
        "    quantiles: tuple = (0.1, 0.5, 0.9)\n",
        "    epochs_baseline: int = 10\n",
        "    learning_rate: float = 1e-3\n",
        "\n",
        "    # Decision-focused costs\n",
        "    cost_holding: float = 1.0\n",
        "    cost_shortage: float = 4.0\n",
        "\n",
        "    # TFT\n",
        "    epochs_tft: int = 5\n",
        "\n",
        "\n",
        "config = Config()\n",
        "print(\"Config:\", config)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2. Load M5 Dataset\n",
        "# =========================\n",
        "\n",
        "DATA_DIR = config.data_dir\n",
        "\n",
        "sales_path = os.path.join(DATA_DIR, \"sales_train_validation.csv\")\n",
        "calendar_path = os.path.join(DATA_DIR, \"calendar.csv\")\n",
        "prices_path = os.path.join(DATA_DIR, \"sell_prices.csv\")\n",
        "\n",
        "assert os.path.exists(sales_path), \"sales_train_validation.csv not found in DATA_DIR\"\n",
        "assert os.path.exists(calendar_path), \"calendar.csv not found in DATA_DIR\"\n",
        "assert os.path.exists(prices_path), \"sell_prices.csv not found in DATA_DIR\"\n",
        "\n",
        "sales = pd.read_csv(sales_path)\n",
        "calendar = pd.read_csv(calendar_path)\n",
        "prices = pd.read_csv(prices_path)\n",
        "\n",
        "print(\"Loaded data:\")\n",
        "print(\" sales:\", sales.shape)\n",
        "print(\" calendar:\", calendar.shape)\n",
        "print(\" prices:\", prices.shape)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3. Feature Engineering (+ log_demand)\n",
        "# =========================\n",
        "\n",
        "# 3.1 Convert wide -> long\n",
        "id_cols = [c for c in sales.columns if not c.startswith(\"d_\")]\n",
        "value_cols = [c for c in sales.columns if c.startswith(\"d_\")]\n",
        "\n",
        "sales_long = sales.melt(\n",
        "    id_vars=id_cols,\n",
        "    value_vars=value_cols,\n",
        "    var_name=\"d\",\n",
        "    value_name=\"demand\"\n",
        ")\n",
        "\n",
        "# 3.2 Merge with calendar (include wm_yr_wk so we can join prices)\n",
        "calendar_small = calendar[[\n",
        "    \"d\",\n",
        "    \"date\",\n",
        "    \"wm_yr_wk\",      # key for sell_prices\n",
        "    \"wday\",\n",
        "    \"month\",\n",
        "    \"year\",\n",
        "    \"event_name_1\",\n",
        "    \"event_type_1\",\n",
        "    \"snap_CA\",\n",
        "    \"snap_TX\",\n",
        "    \"snap_WI\",\n",
        "]]\n",
        "sales_long = sales_long.merge(calendar_small, on=\"d\", how=\"left\")\n",
        "\n",
        "# 3.3 Merge with prices (now wm_yr_wk is present)\n",
        "sales_long = sales_long.merge(\n",
        "    prices,\n",
        "    on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Ensure date is datetime\n",
        "sales_long[\"date\"] = pd.to_datetime(sales_long[\"date\"])\n",
        "sales_long = sales_long.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Long-format sales:\", sales_long.shape)\n",
        "\n",
        "\n",
        "def add_features(df, lags=(1, 7, 28), rolling_windows=(7, 28)):\n",
        "    df = df.copy()\n",
        "    # Temporal features\n",
        "    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
        "    df[\"weekofyear\"] = df[\"date\"].dt.isocalendar().week.astype(int)\n",
        "    df[\"month_num\"] = df[\"date\"].dt.month\n",
        "\n",
        "    # Fill and log price (future-proof: use ffill/bfill)\n",
        "    df[\"sell_price\"] = df[\"sell_price\"].ffill().bfill()\n",
        "    df[\"log_price\"] = np.log1p(df[\"sell_price\"])\n",
        "\n",
        "    # Sort for lags/rolling\n",
        "    df = df.sort_values([\"id\", \"date\"])\n",
        "    group = df.groupby(\"id\", group_keys=False)\n",
        "\n",
        "    # Lags\n",
        "    for lag in lags:\n",
        "        df[f\"lag_{lag}\"] = group[\"demand\"].shift(lag)\n",
        "\n",
        "    # Rolling stats\n",
        "    for window in rolling_windows:\n",
        "        df[f\"rmean_{window}\"] = group[\"demand\"].shift(1).rolling(window).mean()\n",
        "        df[f\"rstd_{window}\"] = group[\"demand\"].shift(1).rolling(window).std()\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "sales_fe = add_features(sales_long)\n",
        "print(\"Feature-engineered sales:\", sales_fe.shape)\n",
        "\n",
        "# Add log_demand as target for both models\n",
        "sales_fe[\"demand\"] = sales_fe[\"demand\"].clip(lower=0.0)\n",
        "sales_fe[\"log_demand\"] = np.log1p(sales_fe[\"demand\"])\n",
        "\n",
        "# Optional: filter to one state for speed\n",
        "STATE_FILTER = config.state_filter\n",
        "if STATE_FILTER is not None:\n",
        "    sales_fe = sales_fe[sales_fe[\"state_id\"] == STATE_FILTER].reset_index(drop=True)\n",
        "    print(f\"Filtered to state {STATE_FILTER}:\", sales_fe.shape)\n",
        "\n",
        "# Encode simple categorical IDs (do this once, before folds)\n",
        "for col in [\"store_id\", \"dept_id\", \"item_id\"]:\n",
        "    sales_fe[col + \"_idx\"] = sales_fe[col].astype(\"category\").cat.codes\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4. Rolling-Origin CV Definition (3 × 28-day folds)\n",
        "# =========================\n",
        "\n",
        "all_dates = np.array(sorted(sales_fe[\"date\"].unique()))\n",
        "test_horizon = 28\n",
        "n_folds = 3\n",
        "\n",
        "folds = []\n",
        "for k in range(n_folds):\n",
        "    # For k=0, test is the oldest of the 3 blocks at the end, etc.\n",
        "    test_start = all_dates[-(n_folds - k) * test_horizon]\n",
        "    if k < n_folds - 1:\n",
        "        test_end = all_dates[-(n_folds - k - 1) * test_horizon - 1]\n",
        "    else:\n",
        "        test_end = all_dates[-1]\n",
        "\n",
        "    train_dates = all_dates[all_dates < test_start]\n",
        "    test_dates = all_dates[(all_dates >= test_start) & (all_dates <= test_end)]\n",
        "\n",
        "    folds.append({\n",
        "        \"train_dates\": train_dates,\n",
        "        \"test_dates\": test_dates,\n",
        "    })\n",
        "\n",
        "    print(\n",
        "        f\"Fold {k+1}: \"\n",
        "        f\"Train {train_dates[0]} → {train_dates[-1]} \"\n",
        "        f\"| Test {test_dates[0]} → {test_dates[-1]} \"\n",
        "        f\"({len(test_dates)} days)\"\n",
        "    )\n",
        "\n",
        "print(\"\\nRolling-origin CV defined with 3 × 28-day folds.\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. PyTorch Dataset + Training Utilities\n",
        "# =========================\n",
        "\n",
        "feature_cols = [\n",
        "    \"dayofweek\",\n",
        "    \"weekofyear\",\n",
        "    \"month_num\",\n",
        "    \"log_price\",\n",
        "    \"lag_1\",\n",
        "    \"lag_7\",\n",
        "    \"lag_28\",\n",
        "    \"rmean_7\",\n",
        "    \"rstd_7\",\n",
        "    \"rmean_28\",\n",
        "    \"rstd_28\",\n",
        "]\n",
        "cat_cols = [\"store_id_idx\", \"dept_id_idx\", \"item_id_idx\"]\n",
        "\n",
        "# IMPORTANT: target is log_demand (for both baseline & DF models)\n",
        "target_col = \"log_demand\"\n",
        "num_cols = feature_cols\n",
        "\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y, ids, dates):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.ids = ids\n",
        "        self.dates = dates\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"features\": torch.tensor(self.X[idx], dtype=torch.float32),\n",
        "            \"target\": torch.tensor(self.y[idx], dtype=torch.float32),  # log_demand\n",
        "            \"id\": self.ids[idx],\n",
        "            \"date\": str(self.dates[idx]),\n",
        "        }\n",
        "\n",
        "\n",
        "def build_datasets_for_fold(fold, df, batch_size):\n",
        "    \"\"\"\n",
        "    For a given fold:\n",
        "      - train_dates (outer) for training + validation window\n",
        "      - test_dates for evaluation\n",
        "    We further split the outer training dates into:\n",
        "      - inner train\n",
        "      - inner validation (last 28 days)\n",
        "    \"\"\"\n",
        "    train_dates_outer = fold[\"train_dates\"]\n",
        "    test_dates = fold[\"test_dates\"]\n",
        "\n",
        "    df_trainval = df[df[\"date\"].isin(train_dates_outer)].copy()\n",
        "    df_test = df[df[\"date\"].isin(test_dates)].copy()\n",
        "\n",
        "    # inner validation window: last 28 days in trainval\n",
        "    inner_dates = np.array(sorted(df_trainval[\"date\"].unique()))\n",
        "    val_window = 28\n",
        "    if len(inner_dates) > val_window:\n",
        "        val_start = inner_dates[-val_window]\n",
        "        inner_train_dates = inner_dates[inner_dates < val_start]\n",
        "        inner_val_dates = inner_dates[inner_dates >= val_start]\n",
        "    else:\n",
        "        # fallback: 80/20 on trainval dates if train history is small\n",
        "        split_idx = int(0.8 * len(inner_dates))\n",
        "        inner_train_dates = inner_dates[:split_idx]\n",
        "        inner_val_dates = inner_dates[split_idx:]\n",
        "\n",
        "    train_df = df_trainval[df_trainval[\"date\"].isin(inner_train_dates)].copy()\n",
        "    val_df = df_trainval[df_trainval[\"date\"].isin(inner_val_dates)].copy()\n",
        "\n",
        "    # scaler fit on numeric cols\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_df[num_cols])\n",
        "\n",
        "    def preprocess_frame(df_):\n",
        "        X_num = scaler.transform(df_[num_cols])\n",
        "        X_cat = df_[cat_cols].values\n",
        "        X = np.concatenate([X_num, X_cat], axis=1)\n",
        "        y = df_[target_col].values.astype(np.float32)  # log_demand\n",
        "        return X.astype(np.float32), y\n",
        "\n",
        "    X_train, y_train = preprocess_frame(train_df)\n",
        "    X_val, y_val = preprocess_frame(val_df)\n",
        "    X_test, y_test = preprocess_frame(df_test)\n",
        "\n",
        "    train_ds = TabularDataset(\n",
        "        X_train, y_train,\n",
        "        train_df[\"id\"].values,\n",
        "        train_df[\"date\"].values,\n",
        "    )\n",
        "    val_ds = TabularDataset(\n",
        "        X_val, y_val,\n",
        "        val_df[\"id\"].values,\n",
        "        val_df[\"date\"].values,\n",
        "    )\n",
        "    test_ds = TabularDataset(\n",
        "        X_test, y_test,\n",
        "        df_test[\"id\"].values,\n",
        "        df_test[\"date\"].values,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return (\n",
        "        train_df,\n",
        "        val_df,\n",
        "        df_test,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        test_loader,\n",
        "        X_train.shape[1],\n",
        "        y_test,\n",
        "        df_test[\"id\"].values,\n",
        "        df_test[\"date\"].values,\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. Baseline Quantile Model (Pinball Loss, log_demand)\n",
        "# =========================\n",
        "\n",
        "def pinball_loss(y_true, y_pred, quantiles):\n",
        "    \"\"\"\n",
        "    y_true: (batch,)  [log_demand]\n",
        "    y_pred: (batch, n_quantiles)  [log_demand]\n",
        "    quantiles: list or tensor of length n_quantiles\n",
        "    \"\"\"\n",
        "    q = torch.tensor(quantiles, device=y_pred.device).view(1, -1)\n",
        "    diff = y_true.unsqueeze(1) - y_pred\n",
        "    loss = torch.maximum(q * diff, (q - 1.0) * diff)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "class QuantileMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, quantiles=(0.1, 0.5, 0.9)):\n",
        "        super().__init__()\n",
        "        self.quantiles = quantiles\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, len(quantiles))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def train_baseline(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    quantiles,\n",
        "    epochs=config.epochs_baseline,\n",
        "    lr=config.learning_rate,\n",
        "):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            x = batch[\"features\"].to(DEVICE)\n",
        "            y_log = batch[\"target\"].to(DEVICE)  # log_demand\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds_log = model(x)  # log-quantiles\n",
        "            loss = pinball_loss(y_log, preds_log, quantiles)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch[\"features\"].to(DEVICE)\n",
        "                y_log = batch[\"target\"].to(DEVICE)\n",
        "                preds_log = model(x)\n",
        "                loss = pinball_loss(y_log, preds_log, quantiles)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_train = float(np.mean(train_losses))\n",
        "        avg_val = float(np.mean(val_losses))\n",
        "        history.append((avg_train, avg_val))\n",
        "        print(f\"[Baseline] Epoch {epoch:02d} | Train Pinball: {avg_train:.4f} | Val Pinball: {avg_val:.4f}\")\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return history\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7. Decision-Focused Model (log-space quantiles, warm-start, multi-task)\n",
        "# =========================\n",
        "\n",
        "def smooth_max(u, lam=10.0):\n",
        "    # Softplus-based smooth approximation to max(u, 0)\n",
        "    return (1.0 / lam) * torch.log1p(torch.exp(lam * u))\n",
        "\n",
        "\n",
        "class DecisionFocusedMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Decision-focused MLP that:\n",
        "    - predicts base quantiles in LOG space\n",
        "    - interpolates to q_hat_log at contextual tau(x)\n",
        "    - back-transforms to original demand via expm1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dim=128,\n",
        "        base_quantiles=(0.1, 0.5, 0.9),\n",
        "        tau_init: float | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Store base quantiles as tensor\n",
        "        self.register_buffer(\"base_quantiles\", torch.tensor(base_quantiles).view(1, -1))\n",
        "\n",
        "        # Shared feature extractor\n",
        "        self.feature_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Head that outputs base quantiles IN LOG SPACE\n",
        "        self.quantile_head = nn.Linear(hidden_dim, len(base_quantiles))\n",
        "\n",
        "        # Contextual service level τ(x) in (0,1)\n",
        "        self.tau_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialise tau around theoretical newsvendor quantile if provided\n",
        "        if tau_init is not None:\n",
        "            with torch.no_grad():\n",
        "                # logit(tau) = log(tau / (1 - tau))\n",
        "                bias = math.log(tau_init / (1.0 - tau_init))\n",
        "                self.tau_head[0].bias.fill_(bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - q_base_log: (batch, Q)  base quantiles in log space\n",
        "        - tau_ctx:    (batch,)    contextual τ(x)\n",
        "        \"\"\"\n",
        "        h = self.feature_net(x)\n",
        "        q_base_log = self.quantile_head(h)\n",
        "        # clamp log-quantiles for numerical stability\n",
        "        q_base_log = torch.clamp(q_base_log, min=-10.0, max=10.0)\n",
        "\n",
        "        tau_ctx = self.tau_head(h).squeeze(-1)\n",
        "        # clamp tau to avoid exactly 0 or 1\n",
        "        tau_ctx = torch.clamp(tau_ctx, 1e-6, 1.0 - 1e-6)\n",
        "        return q_base_log, tau_ctx\n",
        "\n",
        "    def compute_q_hat(self, q_base_log, tau_ctx):\n",
        "        \"\"\"\n",
        "        Interpolate in LOG space to get q_hat_log, then:\n",
        "        - transform to original demand: q_hat = expm1(q_hat_log)\n",
        "        - clamp to q_hat >= 0\n",
        "        \"\"\"\n",
        "        device = q_base_log.device\n",
        "        base_q = self.base_quantiles.to(device)   # (1, Q)\n",
        "        taus = base_q.squeeze(0)                 # (Q,)\n",
        "        Q = taus.shape[0]\n",
        "\n",
        "        # tau_ctx: (batch,)\n",
        "        tau = tau_ctx.clamp(1e-6, 1.0 - 1e-6)\n",
        "        batch_size = q_base_log.size(0)\n",
        "\n",
        "        # Expand to (batch, Q)\n",
        "        tau_exp = tau.unsqueeze(1).expand(-1, Q)\n",
        "        taus_exp = taus.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Masks for bracketing\n",
        "        le_mask = taus_exp <= tau_exp    # taus <= tau\n",
        "        ge_mask = taus_exp >= tau_exp    # taus >= tau\n",
        "\n",
        "        idx_range = torch.arange(Q, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # lower_idx: largest index j with taus[j] <= tau\n",
        "        le_idx_masked = torch.where(le_mask, idx_range, torch.zeros_like(idx_range))\n",
        "        lower_idx = le_idx_masked.max(dim=1).values  # if all False, 0\n",
        "\n",
        "        # upper_idx: smallest index j with taus[j] >= tau\n",
        "        ge_idx_masked = torch.where(\n",
        "            ge_mask,\n",
        "            idx_range,\n",
        "            torch.full_like(idx_range, Q - 1)\n",
        "        )\n",
        "        upper_idx = ge_idx_masked.min(dim=1).values  # if all False, Q-1\n",
        "\n",
        "        # ensure ordering\n",
        "        lower_idx = torch.min(lower_idx, upper_idx)\n",
        "        upper_idx = torch.max(lower_idx, upper_idx)\n",
        "\n",
        "        batch_idx = torch.arange(batch_size, device=device)\n",
        "        lower_q_log = q_base_log[batch_idx, lower_idx]\n",
        "        upper_q_log = q_base_log[batch_idx, upper_idx]\n",
        "        lower_tau = taus[lower_idx]\n",
        "        upper_tau = taus[upper_idx]\n",
        "\n",
        "        eps = 1e-6\n",
        "        w = (tau - lower_tau) / torch.clamp(upper_tau - lower_tau, min=eps)\n",
        "        w = torch.clamp(w, 0.0, 1.0)\n",
        "\n",
        "        # Interpolated log-quantile\n",
        "        q_hat_log = lower_q_log + w * (upper_q_log - lower_q_log)\n",
        "\n",
        "        # Back to original demand space, enforce non-negativity\n",
        "        q_hat = torch.expm1(q_hat_log)\n",
        "        q_hat = torch.clamp(q_hat, min=0.0)\n",
        "        return q_hat\n",
        "\n",
        "\n",
        "def newsvendor_loss(y_true, q_hat, ch=1.0, cs=4.0):\n",
        "    \"\"\"\n",
        "    Stable Newsvendor loss in ORIGINAL demand space.\n",
        "\n",
        "    y_true: actual demand (>= 0)\n",
        "    q_hat:  order quantity from model (>= 0, already clamped)\n",
        "    ch: holding cost per excess unit\n",
        "    cs: shortage cost per unit of underage\n",
        "    \"\"\"\n",
        "    q_hat = torch.clamp(q_hat, min=0.0)\n",
        "\n",
        "    diff = q_hat - y_true\n",
        "    over = torch.relu(diff)    # overstock\n",
        "    under = torch.relu(-diff)  # stockout\n",
        "\n",
        "    cost = ch * over + cs * under\n",
        "    return cost.mean()\n",
        "\n",
        "\n",
        "def train_decision_focused(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    ch=1.0,\n",
        "    cs=4.0,\n",
        "    epochs=10,\n",
        "    lr=1e-3,\n",
        "    alpha=0.3,             # weight on pinball loss (stabilises)\n",
        "    quantiles=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Multi-task training:\n",
        "    - Train loss = alpha * Pinball(log-space) + (1 - alpha) * NV(original space)\n",
        "    - Validation monitored on NV only (decision quality).\n",
        "    \"\"\"\n",
        "    if quantiles is None:\n",
        "        quantiles = config.quantiles\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    best_val_nv = float(\"inf\")\n",
        "    best_state = None\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch[\"features\"].to(DEVICE)\n",
        "            y_log = batch[\"target\"].to(DEVICE)       # log_demand\n",
        "            y_true = torch.expm1(y_log)              # original demand\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            q_base_log, tau_ctx = model(x)\n",
        "            q_hat = model.compute_q_hat(q_base_log, tau_ctx)\n",
        "\n",
        "            # (1) Pinball loss (forecast accuracy in log space)\n",
        "            pin_loss = pinball_loss(y_log, q_base_log, quantiles)\n",
        "\n",
        "            # (2) Newsvendor loss (decision loss in original space)\n",
        "            nv_loss = newsvendor_loss(y_true, q_hat, ch=ch, cs=cs)\n",
        "\n",
        "            loss = alpha * pin_loss + (1.0 - alpha) * nv_loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation: monitor pure NV (decision quality)\n",
        "        model.eval()\n",
        "        val_nv_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch[\"features\"].to(DEVICE)\n",
        "                y_log = batch[\"target\"].to(DEVICE)\n",
        "                y_true = torch.expm1(y_log)\n",
        "\n",
        "                q_base_log, tau_ctx = model(x)\n",
        "                q_hat = model.compute_q_hat(q_base_log, tau_ctx)\n",
        "                nv = newsvendor_loss(y_true, q_hat, ch=ch, cs=cs)\n",
        "                val_nv_losses.append(nv.item())\n",
        "\n",
        "        avg_train = float(np.mean(train_losses))\n",
        "        avg_val_nv = float(np.mean(val_nv_losses))\n",
        "        history.append((avg_train, avg_val_nv))\n",
        "        print(f\"[DF] Epoch {epoch:02d} | Train MixLoss: {avg_train:.4f} | Val NV: {avg_val_nv:.4f}\")\n",
        "\n",
        "        if not np.isnan(avg_val_nv) and avg_val_nv < best_val_nv:\n",
        "            best_val_nv = avg_val_nv\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    # Fallback if best_state was never updated (e.g., all NaNs)\n",
        "    if best_state is None:\n",
        "        best_state = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return history\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 8. Rolling Evaluation Loop\n",
        "# =========================\n",
        "\n",
        "def predict_baseline(model, loader, quantiles):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      y_true:   original demand (expm1(log_demand))\n",
        "      q_pred:   predicted quantiles in original demand space\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_y, all_q, all_ids, all_dates = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            x = batch[\"features\"].to(DEVICE)\n",
        "            y_log = batch[\"target\"].to(DEVICE)  # log_demand\n",
        "            y = torch.expm1(y_log)              # original demand\n",
        "\n",
        "            preds_log = model(x)                # log-quantiles\n",
        "            preds = torch.expm1(preds_log)      # original demand quantiles\n",
        "\n",
        "            all_y.append(y.cpu().numpy())\n",
        "            all_q.append(preds.cpu().numpy())\n",
        "            all_ids.append(batch[\"id\"])\n",
        "            all_dates.append(batch[\"date\"])\n",
        "    y_true = np.concatenate(all_y)\n",
        "    q_pred = np.concatenate(all_q)\n",
        "    ids = np.concatenate(all_ids)\n",
        "    dates = np.concatenate(all_dates)\n",
        "    return y_true, q_pred, ids, dates\n",
        "\n",
        "\n",
        "def predict_decision_focused(model, loader):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      y_true:  original demand\n",
        "      q_hat:   DF order quantities (original demand)\n",
        "      tau:     learned service levels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_y, all_qhat, all_tau, all_ids, all_dates = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            x = batch[\"features\"].to(DEVICE)\n",
        "            y_log = batch[\"target\"].to(DEVICE)  # log_demand\n",
        "            y_true = torch.expm1(y_log)\n",
        "\n",
        "            q_base_log, tau_ctx = model(x)\n",
        "            q_hat = model.compute_q_hat(q_base_log, tau_ctx)\n",
        "\n",
        "            all_y.append(y_true.cpu().numpy())\n",
        "            all_qhat.append(q_hat.cpu().numpy())\n",
        "            all_tau.append(tau_ctx.cpu().numpy())\n",
        "            all_ids.append(batch[\"id\"])\n",
        "            all_dates.append(batch[\"date\"])\n",
        "\n",
        "    y_true = np.concatenate(all_y)\n",
        "    q_hat = np.concatenate(all_qhat)\n",
        "    tau = np.concatenate(all_tau)\n",
        "    ids = np.concatenate(all_ids)\n",
        "    dates = np.concatenate(all_dates)\n",
        "    return y_true, q_hat, tau, ids, dates\n",
        "\n",
        "\n",
        "def mase(y_true, y_pred, seasonality=7):\n",
        "    y = pd.Series(y_true)\n",
        "    if len(y) <= seasonality:\n",
        "        return np.nan\n",
        "    naive = y.shift(seasonality).dropna()\n",
        "    aligned = y.iloc[seasonality:]\n",
        "    mae_naive = np.mean(np.abs(aligned.values - naive.values))\n",
        "    return float(np.mean(np.abs(y_pred - y_true)) / (mae_naive + 1e-8))\n",
        "\n",
        "\n",
        "def rmsse(y_true, y_pred, seasonality=7):\n",
        "    y = pd.Series(y_true)\n",
        "    if len(y) <= seasonality:\n",
        "        return np.nan\n",
        "    naive = y.shift(seasonality).dropna()\n",
        "    aligned = y.iloc[seasonality:]\n",
        "    mse_naive = np.mean((aligned.values - naive.values) ** 2)\n",
        "    return float(math.sqrt(np.mean((y_pred - y_true) ** 2) / (mse_naive + 1e-8)))\n",
        "\n",
        "\n",
        "def decision_metrics(y_true, q, ch=1.0, cs=4.0):\n",
        "    y_true = np.asarray(y_true)\n",
        "    q = np.asarray(q)\n",
        "    over = np.maximum(q - y_true, 0.0)\n",
        "    under = np.maximum(y_true - q, 0.0)\n",
        "    cost = ch * over + cs * under\n",
        "    sales_served = np.minimum(q, y_true)\n",
        "    fill_rate = float(sales_served.sum() / (y_true.sum() + 1e-8))\n",
        "    stockouts = float((q < y_true).mean())\n",
        "    return {\n",
        "        \"expected_cost\": float(cost.mean()),\n",
        "        \"fill_rate\": fill_rate,\n",
        "        \"stockout_freq\": stockouts,\n",
        "    }\n",
        "\n",
        "\n",
        "def quantile_trapz_weights(taus):\n",
        "    taus = np.array(taus)\n",
        "    Q = len(taus)\n",
        "    w = np.zeros(Q)\n",
        "    for k in range(Q):\n",
        "        if k == 0:\n",
        "            w[k] = (taus[1] - taus[0]) / 2.0\n",
        "        elif k == Q - 1:\n",
        "            w[k] = (taus[-1] - taus[-2]) / 2.0\n",
        "        else:\n",
        "            w[k] = (taus[k + 1] - taus[k - 1]) / 2.0\n",
        "    return w\n",
        "\n",
        "\n",
        "def crps_from_quantiles(y_true, q_pred, quantiles):\n",
        "    \"\"\"\n",
        "    Approximate CRPS in ORIGINAL demand space,\n",
        "    given quantiles q_pred (original demand).\n",
        "    \"\"\"\n",
        "    y = np.asarray(y_true).reshape(-1, 1)\n",
        "    q_pred = np.asarray(q_pred)\n",
        "    taus = np.array(quantiles).reshape(1, -1)\n",
        "    diff = y - q_pred\n",
        "    losses = np.maximum(taus * diff, (taus - 1.0) * diff)\n",
        "    weights = quantile_trapz_weights(quantiles).reshape(1, -1)\n",
        "    crps = 2.0 * np.sum(losses * weights, axis=1)\n",
        "    return float(crps.mean())\n",
        "\n",
        "\n",
        "def crps_point_mass(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    CRPS for a deterministic forecast equals MAE,\n",
        "    computed in ORIGINAL demand space.\n",
        "    \"\"\"\n",
        "    return float(np.mean(np.abs(np.asarray(y_true) - np.asarray(y_pred))))\n",
        "\n",
        "\n",
        "baseline_quantiles = config.quantiles\n",
        "fold_results = []\n",
        "\n",
        "for fold_idx, fold in enumerate(folds):\n",
        "    print(f\"\\n=========== FOLD {fold_idx+1}/{n_folds} ===========\")\n",
        "\n",
        "    (\n",
        "        train_df_fold,\n",
        "        val_df_fold,\n",
        "        test_df_fold,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        test_loader,\n",
        "        input_dim,\n",
        "        y_test_true_log,\n",
        "        ids_test,\n",
        "        dates_test,\n",
        "    ) = build_datasets_for_fold(\n",
        "        fold,\n",
        "        sales_fe,\n",
        "        batch_size=config.batch_size,\n",
        "    )\n",
        "\n",
        "    # Baseline model (on log_demand)\n",
        "    baseline_model = QuantileMLP(\n",
        "        input_dim,\n",
        "        hidden_dim=128,\n",
        "        quantiles=baseline_quantiles,\n",
        "    ).to(DEVICE)\n",
        "    print(\"Training baseline quantile model...\")\n",
        "    baseline_history = train_baseline(\n",
        "        baseline_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        baseline_quantiles,\n",
        "        epochs=config.epochs_baseline,\n",
        "        lr=config.learning_rate,\n",
        "    )\n",
        "\n",
        "    # Decision-focused model: warm-start from baseline + tau_init at theoretical τ*\n",
        "    tau_star = config.cost_shortage / (config.cost_shortage + config.cost_holding)\n",
        "\n",
        "    df_model = DecisionFocusedMLP(\n",
        "        input_dim,\n",
        "        hidden_dim=128,\n",
        "        base_quantiles=baseline_quantiles,\n",
        "        tau_init=tau_star,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Warm-start weights: copy baseline MLP into DF feature & quantile heads\n",
        "    with torch.no_grad():\n",
        "        # baseline.net: [Linear, ReLU, Linear, ReLU, Linear]\n",
        "        # df_model.feature_net: [Linear, ReLU, Linear, ReLU]\n",
        "        df_model.feature_net[0].weight.copy_(baseline_model.net[0].weight)\n",
        "        df_model.feature_net[0].bias.copy_(baseline_model.net[0].bias)\n",
        "        df_model.feature_net[2].weight.copy_(baseline_model.net[2].weight)\n",
        "        df_model.feature_net[2].bias.copy_(baseline_model.net[2].bias)\n",
        "        df_model.quantile_head.weight.copy_(baseline_model.net[4].weight)\n",
        "        df_model.quantile_head.bias.copy_(baseline_model.net[4].bias)\n",
        "\n",
        "    print(\"Training decision-focused model (multi-task: Pinball + NV)...\")\n",
        "    df_history = train_decision_focused(\n",
        "        df_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        ch=config.cost_holding,\n",
        "        cs=config.cost_shortage,\n",
        "        epochs=config.epochs_baseline,\n",
        "        lr=config.learning_rate,\n",
        "        alpha=0.3,\n",
        "        quantiles=baseline_quantiles,\n",
        "    )\n",
        "\n",
        "    # Evaluate baseline on this fold (ORIGINAL demand)\n",
        "    y_test_true_b, q_pred_test, ids_b, dates_b = predict_baseline(\n",
        "        baseline_model,\n",
        "        test_loader,\n",
        "        baseline_quantiles,\n",
        "    )\n",
        "    median_pred = q_pred_test[:, 1]  # 0.5 quantile\n",
        "\n",
        "    baseline_mase = mase(y_test_true_b, median_pred)\n",
        "    baseline_rmsse = rmsse(y_test_true_b, median_pred)\n",
        "    baseline_dec = decision_metrics(\n",
        "        y_test_true_b,\n",
        "        median_pred,\n",
        "        ch=config.cost_holding,\n",
        "        cs=config.cost_shortage,\n",
        "    )\n",
        "    baseline_crps = crps_from_quantiles(\n",
        "        y_test_true_b,\n",
        "        q_pred_test,\n",
        "        baseline_quantiles,\n",
        "    )\n",
        "\n",
        "    print(\"\\nBASELINE (Pinball, log_demand) – Fold\", fold_idx + 1)\n",
        "    print(f\"  MASE:  {baseline_mase:.4f}\")\n",
        "    print(f\"  RMSSE: {baseline_rmsse:.4f}\")\n",
        "    print(f\"  Expected Cost: {baseline_dec['expected_cost']:.4f}\")\n",
        "    print(f\"  Fill Rate:     {baseline_dec['fill_rate']:.4f}\")\n",
        "    print(f\"  Stockout Freq: {baseline_dec['stockout_freq']:.4f}\")\n",
        "    print(f\"  CRPS:          {baseline_crps:.4f}\")\n",
        "\n",
        "    # Evaluate decision-focused model on this fold (ORIGINAL demand)\n",
        "    y_df_true, q_hat_test, tau_test, ids_df, dates_df = predict_decision_focused(\n",
        "        df_model,\n",
        "        test_loader,\n",
        "    )\n",
        "    df_mase = mase(y_df_true, q_hat_test)\n",
        "    df_rmsse = rmsse(y_df_true, q_hat_test)\n",
        "    df_dec = decision_metrics(\n",
        "        y_df_true,\n",
        "        q_hat_test,\n",
        "        ch=config.cost_holding,\n",
        "        cs=config.cost_shortage,\n",
        "    )\n",
        "    df_crps = crps_point_mass(y_df_true, q_hat_test)\n",
        "\n",
        "    print(\"\\nDECISION-FOCUSED (Newsvendor) – Fold\", fold_idx + 1)\n",
        "    print(f\"  MASE:  {df_mase:.4f}\")\n",
        "    print(f\"  RMSSE: {df_rmsse:.4f}\")\n",
        "    print(f\"  Expected Cost: {df_dec['expected_cost']:.4f}\")\n",
        "    print(f\"  Fill Rate:     {df_dec['fill_rate']:.4f}\")\n",
        "    print(f\"  Stockout Freq: {df_dec['stockout_freq']:.4f}\")\n",
        "    print(f\"  CRPS (MAE):    {df_crps:.4f}\")\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold_idx + 1,\n",
        "        \"baseline_mase\": baseline_mase,\n",
        "        \"baseline_rmsse\": baseline_rmsse,\n",
        "        \"baseline_cost\": baseline_dec[\"expected_cost\"],\n",
        "        \"baseline_fill\": baseline_dec[\"fill_rate\"],\n",
        "        \"baseline_stockout\": baseline_dec[\"stockout_freq\"],\n",
        "        \"baseline_crps\": baseline_crps,\n",
        "        \"df_mase\": df_mase,\n",
        "        \"df_rmsse\": df_rmsse,\n",
        "        \"df_cost\": df_dec[\"expected_cost\"],\n",
        "        \"df_fill\": df_dec[\"fill_rate\"],\n",
        "        \"df_stockout\": df_dec[\"stockout_freq\"],\n",
        "        \"df_crps\": df_crps,\n",
        "    })\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "fold_results_df = pd.DataFrame(fold_results)\n",
        "print(\"\\n========== CROSS-FOLD SUMMARY ==========\")\n",
        "print(fold_results_df)\n",
        "\n",
        "print(\"\\nMEAN OVER 3 FOLDS:\")\n",
        "print(fold_results_df.mean(numeric_only=True))\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 9. Simple Hierarchical Aggregation (Bottom-Up)\n",
        "# =========================\n",
        "# (Use last fold's baseline predictions as an example)\n",
        "\n",
        "if len(fold_results) > 0:\n",
        "    df_baseline_pred = pd.DataFrame({\n",
        "        \"id\": ids_b,\n",
        "        \"date\": dates_b,\n",
        "        \"y_true\": y_test_true_b,\n",
        "        \"q50\": median_pred,\n",
        "    })\n",
        "\n",
        "    hier = sales_fe[[\"id\", \"store_id\", \"dept_id\", \"item_id\"]].drop_duplicates(\"id\")\n",
        "    df_baseline_pred = df_baseline_pred.merge(hier, on=\"id\", how=\"left\")\n",
        "\n",
        "    agg_dept_store = df_baseline_pred.groupby(\n",
        "        [\"store_id\", \"dept_id\", \"date\"], as_index=False\n",
        "    )[[\"y_true\", \"q50\"]].sum()\n",
        "\n",
        "    print(\"\\nAggregated (store, dept, date) sample:\")\n",
        "    print(agg_dept_store.head())\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 10. Cost Sensitivity Analysis (last fold illustration)\n",
        "# =========================\n",
        "\n",
        "def cost_sensitivity(y_true, baseline_q, df_qhat, ch=1.0, cs_list=(1, 2, 4, 8, 10)):\n",
        "    rows = []\n",
        "    for cs in cs_list:\n",
        "        base_dec = decision_metrics(y_true, baseline_q, ch=ch, cs=cs)\n",
        "        df_dec = decision_metrics(y_true, df_qhat, ch=ch, cs=cs)\n",
        "        rows.append({\n",
        "            \"cs\": cs,\n",
        "            \"baseline_cost\": base_dec[\"expected_cost\"],\n",
        "            \"df_cost\": df_dec[\"expected_cost\"],\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "if len(fold_results) > 0:\n",
        "    cs_list = [1, 2, 4, 6, 8, 10]\n",
        "    sens_df = cost_sensitivity(\n",
        "        y_test_true_b,\n",
        "        median_pred,\n",
        "        q_hat_test,\n",
        "        ch=1.0,\n",
        "        cs_list=cs_list,\n",
        "    )\n",
        "    print(\"\\nCost sensitivity table (last fold):\")\n",
        "    print(sens_df)\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(sens_df[\"cs\"], sens_df[\"baseline_cost\"], marker=\"o\", label=\"Baseline (Pinball)\")\n",
        "    plt.plot(sens_df[\"cs\"], sens_df[\"df_cost\"], marker=\"o\", label=\"Decision-Focused\")\n",
        "    plt.xlabel(\"Shortage Cost cs (holding cost ch = 1)\")\n",
        "    plt.ylabel(\"Expected Inventory Cost\")\n",
        "    plt.title(\"Cost Sensitivity to cs:ch Ratio (last fold)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 11. Temporal Fusion Transformer (TFT) Section (optional)\n",
        "# =========================\n",
        "# In Colab, first run:\n",
        "#   !pip install pytorch-lightning pytorch-forecasting --quiet\n",
        "\n",
        "try:\n",
        "    from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "    from pytorch_forecasting.metrics import QuantileLoss as TFTQuantileLoss\n",
        "    from pytorch_lightning import Trainer, seed_everything\n",
        "\n",
        "    tft_available = True\n",
        "except Exception as e:\n",
        "    print(\"\\n[Warning] pytorch-forecasting / pytorch-lightning not available. \"\n",
        "          \"Install them in Colab to run the TFT section.\")\n",
        "    print(\" Error:\", e)\n",
        "    tft_available = False\n",
        "\n",
        "\n",
        "if tft_available:\n",
        "    seed_everything(SEED)\n",
        "\n",
        "    # Use the LARGEST training window (from the last fold)\n",
        "    tft_train_dates = folds[-1][\"train_dates\"]\n",
        "\n",
        "    tft_df = sales_fe.copy()\n",
        "    tft_df = tft_df.sort_values(\"date\")\n",
        "    tft_df[\"time_idx\"] = tft_df[\"date\"].rank(method=\"dense\").astype(int)\n",
        "\n",
        "    # Reduce number of series for speed (optional)\n",
        "    N_SERIES = 200\n",
        "    tmp_train = tft_df[tft_df[\"date\"].isin(tft_train_dates)]\n",
        "    top_ids = (\n",
        "        tmp_train.groupby(\"id\")[\"demand\"]\n",
        "        .sum()\n",
        "        .sort_values(ascending=False)\n",
        "        .head(N_SERIES)\n",
        "        .index\n",
        "    )\n",
        "    tft_df = tft_df[tft_df[\"id\"].isin(top_ids)].reset_index(drop=True)\n",
        "\n",
        "    max_time_idx = tft_df[\"time_idx\"].max()\n",
        "    training_cutoff = int(0.8 * max_time_idx)\n",
        "\n",
        "    max_encoder_length = 28   # past context\n",
        "    max_prediction_length = 7 # forecast horizon\n",
        "\n",
        "    tft_training = TimeSeriesDataSet(\n",
        "        tft_df[lambda x: x.time_idx <= training_cutoff],\n",
        "        time_idx=\"time_idx\",\n",
        "        target=\"demand\",  # TFT still on original demand\n",
        "        group_ids=[\"id\"],\n",
        "        static_categoricals=[\"id\"],\n",
        "        time_varying_known_reals=[\"time_idx\", \"sell_price\"],\n",
        "        time_varying_unknown_reals=[\"demand\"],\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "    )\n",
        "\n",
        "    tft_validation = TimeSeriesDataSet.from_dataset(\n",
        "        tft_training,\n",
        "        tft_df,\n",
        "        min_prediction_idx=training_cutoff + 1,\n",
        "    )\n",
        "\n",
        "    tft_train_loader = tft_training.to_dataloader(\n",
        "        train=True,\n",
        "        batch_size=64,\n",
        "        num_workers=0\n",
        "    )\n",
        "    tft_val_loader = tft_validation.to_dataloader(\n",
        "        train=False,\n",
        "        batch_size=64,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        tft_training,\n",
        "        learning_rate=1e-3,\n",
        "        hidden_size=16,\n",
        "        attention_head_size=1,\n",
        "        dropout=0.1,\n",
        "        hidden_continuous_size=8,\n",
        "        output_size=7,  # number of quantiles\n",
        "        loss=TFTQuantileLoss(),\n",
        "        log_interval=10,\n",
        "        log_val_interval=1,\n",
        "        reduce_on_plateau_patience=2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=config.epochs_tft,\n",
        "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "        devices=1,\n",
        "        enable_model_summary=False,\n",
        "        gradient_clip_val=0.1,\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining Temporal Fusion Transformer (TFT)...\")\n",
        "    trainer.fit(tft, train_dataloaders=tft_train_loader, val_dataloaders=tft_val_loader)\n",
        "\n",
        "    raw_predictions, x_tft = tft.predict(tft_val_loader, mode=\"raw\", return_x=True)\n",
        "\n",
        "    pred_quantiles = raw_predictions[\"prediction\"]\n",
        "    if isinstance(pred_quantiles, list):\n",
        "        pred_quantiles = torch.cat(pred_quantiles, dim=0)\n",
        "\n",
        "    # (N, max_prediction_length, Q)\n",
        "    tft_q = pred_quantiles[:, 0, :]  # 1-step ahead\n",
        "    tft_q_np = tft_q.detach().cpu().numpy()\n",
        "\n",
        "    tft_y = x_tft[\"decoder_target\"][:, 0]  # 1-step ahead true demand\n",
        "    tft_y_np = tft_y.detach().cpu().numpy()\n",
        "\n",
        "    q_levels = np.array(tft.loss.quantiles)\n",
        "    median_idx = np.argmin(np.abs(q_levels - 0.5))\n",
        "    tft_median = tft_q_np[:, median_idx]\n",
        "\n",
        "    tft_mase = mase(tft_y_np, tft_median)\n",
        "    tft_rmsse = rmsse(tft_y_np, tft_median)\n",
        "    tft_decision = decision_metrics(\n",
        "        tft_y_np,\n",
        "        tft_median,\n",
        "        ch=config.cost_holding,\n",
        "        cs=config.cost_shortage,\n",
        "    )\n",
        "    tft_crps = crps_from_quantiles(tft_y_np, tft_q_np, q_levels)\n",
        "\n",
        "    print(\"\\nTEMPORAL FUSION TRANSFORMER (TFT) – Validation (1-step ahead)\")\n",
        "    print(f\"  MASE:  {tft_mase:.4f}\")\n",
        "    print(f\"  RMSSE: {tft_rmsse:.4f}\")\n",
        "    print(f\"  Expected Cost: {tft_decision['expected_cost']:.4f}\")\n",
        "    print(f\"  Fill Rate:     {tft_decision['fill_rate']:.4f}\")\n",
        "    print(f\"  Stockout Freq: {tft_decision['stockout_freq']:.4f}\")\n",
        "    print(f\"  CRPS (quantile-based): {tft_crps:.4f}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 12. Conclusion\n",
        "# =========================\n",
        "\n",
        "print(\"\\nRolling-origin CV notebook finished. Use the fold-wise metrics and \"\n",
        "      \"their averages as your main empirical evidence in the report.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw5tmDhveCR6",
        "outputId": "c6c3d463-5a7e-4c4e-a931-f035e137e400"
      },
      "id": "Yw5tmDhveCR6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Config: Config(data_dir='/content/drive/MyDrive/m5_forecast', state_filter='CA', batch_size=1024, quantiles=(0.1, 0.5, 0.9), epochs_baseline=10, learning_rate=0.001, cost_holding=1.0, cost_shortage=4.0, epochs_tft=5)\n",
            "Loaded data:\n",
            " sales: (30490, 1919)\n",
            " calendar: (1969, 14)\n",
            " prices: (6841121, 4)\n",
            "Long-format sales: (58327370, 19)\n",
            "Feature-engineered sales: (4603990, 30)\n",
            "Filtered to state CA: (1841596, 31)\n",
            "Fold 1: Train 2011-03-09 00:00:00 → 2013-05-12 00:00:00 | Test 2013-05-27 00:00:00 → 2014-05-05 00:00:00 (28 days)\n",
            "Fold 2: Train 2011-03-09 00:00:00 → 2014-05-05 00:00:00 | Test 2014-05-11 00:00:00 → 2015-04-12 00:00:00 (28 days)\n",
            "Fold 3: Train 2011-03-09 00:00:00 → 2015-04-12 00:00:00 | Test 2015-05-05 00:00:00 → 2016-03-27 00:00:00 (28 days)\n",
            "\n",
            "Rolling-origin CV defined with 3 × 28-day folds.\n",
            "\n",
            "=========== FOLD 1/3 ===========\n",
            "Training baseline quantile model...\n",
            "[Baseline] Epoch 01 | Train Pinball: 1.1004 | Val Pinball: 0.8681\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}